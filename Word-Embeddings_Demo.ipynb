{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using Jupyter notebooks\n",
    "# pushing CTRL-c will run the code in a cell\n",
    "2 + 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Gentle Introduction to NLP through Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "![NLP](images/NLP.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# How To Tell If Two Words Are \"Similar\"?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "![distance](images/distance_measures.png)\n",
    "http://dataaspirant.com/2015/04/11/five-most-popular-similarity-measures-implementation-in-python/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Cosine Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "![cos_sim](images/cos_sim.png)\n",
    "http://dataaspirant.com/2015/04/11/five-most-popular-similarity-measures-implementation-in-python/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## calculating dot product\n",
    "$vector_a = [1,2,3]$ <br>\n",
    "$vector_b = [4,5,6]$ <br>\n",
    "$vector_a \\cdot vector_b = (1*4) + (2*5) + (3*6) = 4 + 10 + 18 = 32$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Normalizing a Vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "To normalize a vector, we shrink all values so they fall between $0$ and $1$.\n",
    "\n",
    "$vector_{normalized} = \\frac{vector}{\\sqrt{vector \\cdot vector}}$ <img src=\"images/normalize.jpg\",width=400,height=400>\n",
    "\n",
    "http://www.wikihow.com/Normalize-a-Vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<img src=\"images/unit_circle.png\",width=600,height=600>\n",
    "https://en.wikipedia.org/wiki/Unit_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from nltk.corpus import wordnet\n",
    "from collections import OrderedDict\n",
    "from itertools import combinations\n",
    "import string\n",
    "from gensim import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def normalize_vector(vector):\n",
    "    \"\"\"\n",
    "    Normalizes a vector so that all its values are between 0 and 1\n",
    "    :param vector: a `numpy` vector\n",
    "    :return: a normalized `numpy` vector\n",
    "    \"\"\"\n",
    "    # norm = np.sqrt(vector.dot(vector))\n",
    "    # numpy has a built in function\n",
    "    norm = np.linalg.norm(vector)\n",
    "    if norm:\n",
    "        return vector / norm\n",
    "    else:\n",
    "        # if norm == 0, then original vector was all 0s\n",
    "        return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original vector [1 2 4]\n",
      "normalized vector [ 0.21821789  0.43643578  0.87287156]\n"
     ]
    }
   ],
   "source": [
    "vector_3d = np.array([1,2,4])\n",
    "print(\"original vector\", vector_3d)\n",
    "print(\"normalized vector\", normalize_vector(vector_3d))\n",
    "#0.218 is 1/4th of .873 just like 1 is 1/4th of 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Calculating Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def cos_sim(vector_one, vector_two):\n",
    "    \"\"\"\n",
    "    Calculate the cosine similarity of two `numpy` vectors\n",
    "    :param vector_one: a `numpy` vector\n",
    "    :param vector_two: a `numpy` vector\n",
    "    :return: A score between 0 and 1\n",
    "    \"\"\"\n",
    "    # ensure that both vectors are already normalized\n",
    "    vector_one_norm = normalize_vector(vector_one)\n",
    "    vector_two_norm = normalize_vector(vector_two)\n",
    "    \n",
    "    # calculate the dot product between the two normalized vectors\n",
    "    return vector_one_norm.dot(vector_two_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine similarity of vector_one and vector_two 0.948683298051\n",
      "cosine similarity of vector_one and vector_three 0.904534033733\n",
      "cosine similarity of vector_one and vector_four 0.904534033733\n"
     ]
    }
   ],
   "source": [
    "vector_one = np.array([1,1,1,1,1])\n",
    "vector_two = np.array([1,1,1,1,2])\n",
    "vector_three = np.array([1,2,3,4,5])\n",
    "vector_four = np.array([10,20,30,40,50])\n",
    "\n",
    "print(\"cosine similarity of vector_one and vector_two\", cos_sim(vector_one, vector_two))\n",
    "print(\"cosine similarity of vector_one and vector_three\", cos_sim(vector_one, vector_three))\n",
    "print(\"cosine similarity of vector_one and vector_four\", cos_sim(vector_one, vector_four))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Measuring the \"Similarity\" of Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<img src=\"images/cos_sim_compare.png\",xwidth=1000,height=1000>\n",
    "https://medium.com/@camrongodbout/creating-a-search-engine-f2f429cab33c#.z7i9w8y5t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<img src=\"images/vectorize.png\", width=1000,height=1000>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Option 1: One-hot vectors\n",
    "\n",
    "<img src=\"images/one_hot.png\", width=1000,height=1000>\n",
    "https://blog.acolyer.org/2016/04/21/the-amazing-power-of-word-vectors/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "vocabulary = ['apple', 'banana', 'orange', 'cantaloupe', 'peach']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# generate vocabulary lookup\n",
    "def build_voc_lookup(list_of_voc):\n",
    "    \"\"\"\n",
    "    Generates a dictionary where the key is the word and the value is its index\n",
    "    :param list_of_voc: list of vocabulary words\n",
    "    :return: Dictionary of vocabulary\n",
    "    \"\"\"\n",
    "    lookup_dict = OrderedDict()\n",
    "    counter = 0\n",
    "    for word in list_of_voc:\n",
    "        lookup_dict[word] = counter\n",
    "        counter+=1\n",
    "    return lookup_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# lookup word\n",
    "def lookup_word(lookup_dict, word):\n",
    "    \"\"\" \n",
    "    Looks up a given word in the vocabulary dictionary, and returns None if word not in vocabulary\n",
    "    :param lookup_dict: lookup-dictionary built with build_voc_lookup()\n",
    "    :param word to index\n",
    "    :return: index of word in vocabulary or None\n",
    "    \"\"\"\n",
    "    if word in lookup_dict:\n",
    "        return lookup_dict[word]\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "lookup_dict = build_voc_lookup(vocabulary)\n",
    "print(lookup_word(lookup_dict, 'peach'))\n",
    "print(lookup_word(lookup_dict, 'hashbrown'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# build one-hot vector for word\n",
    "def make_one_hot(lookup_dict, word):\n",
    "    \"\"\"\n",
    "    Builds a one-hot numpy vector for a word\n",
    "    :param lookup_dict: lookup-dictionary built with build_voc_lookup()\n",
    "    :param word: word to convert to one-hot\n",
    "    :return numpy vector with dimension equal to size of vocabulary\n",
    "    \"\"\"\n",
    "    # get size of vocabulary\n",
    "    voc_size = len(lookup_dict.items())\n",
    "    # initialize empty vector of zeros with the size of the vocabulary\n",
    "    one_hot = np.zeros((voc_size))\n",
    "    # get index of word (or None if not in vocabulary)\n",
    "    word_index = lookup_word(lookup_dict, word)\n",
    "    # make the nth dimension of one-hot (representing the index of word in vocabulary) to 1\n",
    "    if word_index or word_index == 0:\n",
    "        one_hot[word_index] = 1\n",
    "    # if word not in vocabulary, the one-hot will remain zeros\n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one-hot vector for '      apple' [ 1.  0.  0.  0.  0.]\n",
      "one-hot vector for '     banana' [ 0.  1.  0.  0.  0.]\n",
      "one-hot vector for '     orange' [ 0.  0.  1.  0.  0.]\n",
      "one-hot vector for ' cantaloupe' [ 0.  0.  0.  1.  0.]\n",
      "one-hot vector for '      peach' [ 0.  0.  0.  0.  1.]\n",
      "one-hot vector for '  hashbrown' [ 0.  0.  0.  0.  0.]\n",
      "one-hot vector for '    Capizzi' [ 0.  0.  0.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "for word in vocabulary + ['hashbrown', 'Capizzi']:\n",
    "    print(\"one-hot vector for '{:>11}'\".format(word), make_one_hot(lookup_dict, word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "#### The problem with one-hot vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine similarity between apple and banana 0.0\n",
      "cosine similarity between apple and orange 0.0\n",
      "cosine similarity between apple and cantaloupe 0.0\n",
      "cosine similarity between apple and peach 0.0\n",
      "cosine similarity between apple and Phoenix 0.0\n",
      "cosine similarity between banana and orange 0.0\n",
      "cosine similarity between banana and cantaloupe 0.0\n",
      "cosine similarity between banana and peach 0.0\n",
      "cosine similarity between banana and Phoenix 0.0\n",
      "cosine similarity between orange and cantaloupe 0.0\n",
      "cosine similarity between orange and peach 0.0\n",
      "cosine similarity between orange and Phoenix 0.0\n",
      "cosine similarity between cantaloupe and peach 0.0\n",
      "cosine similarity between cantaloupe and Phoenix 0.0\n",
      "cosine similarity between peach and Phoenix 0.0\n"
     ]
    }
   ],
   "source": [
    "# add an OOV word to vocabulary\n",
    "vocabulary_plus_oov = vocabulary + [\"Phoenix\"]\n",
    "# get all combinations\n",
    "all_combinations = combinations(vocabulary_plus_oov, 2)\n",
    "# iterate through all combinations and calculate cosine similarity\n",
    "for (word1, word2) in all_combinations:\n",
    "    one_hot_word_1 = make_one_hot(lookup_dict, word1)\n",
    "    one_hot_word_2 = make_one_hot(lookup_dict, word2)\n",
    "    print(\"cosine similarity between {} and {}\".format(word1, word2), cos_sim(one_hot_word_1, one_hot_word_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Option 2: Encode spelling\n",
    "Following a similar pattern as the one-hot of a word over a vocabulary, let's build word vectors represented by the frequency of the letters present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "alphabet = list(string.ascii_lowercase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# since we don't need to worry about \"out-of-vocabulary\" now, we can just use alphabet.index([letter])\n",
    "def lookup_letter(letter):\n",
    "    return alphabet.index(letter.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a 0\n",
      "A 0\n"
     ]
    }
   ],
   "source": [
    "print(\"a\", lookup_letter('a'))\n",
    "print(\"A\", lookup_letter('A'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def make_spelling_vector(word):\n",
    "    \"\"\"\n",
    "    Converts a word into a vector of dimension 26 where each cell contains the count for that letter\n",
    "    :param word: word to vectorize\n",
    "    :return: numpy vector of 26 dimensions\n",
    "    \"\"\"\n",
    "    # initialize vector with zeros\n",
    "    spelling_vector = np.zeros((26))\n",
    "    # iterate through each letter and update count\n",
    "    for letter in word:\n",
    "        if letter in string.ascii_letters:\n",
    "            letter_index = lookup_letter(letter)\n",
    "            spelling_vector[letter_index] = spelling_vector[letter_index] + 1\n",
    "    return spelling_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,\n",
       "        0.,  0.,  2.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_spelling_vector(\"apple\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True], dtype=bool)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# what if two words share the same letters?\n",
    "dog = make_spelling_vector(\"dog\")\n",
    "god = make_spelling_vector(\"God\")\n",
    "god == dog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine similarity between apple and banana 0.303045763366\n",
      "cosine similarity between apple and orange 0.308606699924\n",
      "cosine similarity between apple and cantaloupe 0.654653670708\n",
      "cosine similarity between apple and peach 0.676123403783\n",
      "cosine similarity between apple and Phoenix 0.428571428571\n",
      "cosine similarity between banana and orange 0.54554472559\n",
      "cosine similarity between banana and cantaloupe 0.617213399848\n",
      "cosine similarity between banana and peach 0.3585685828\n",
      "cosine similarity between banana and Phoenix 0.20203050891\n",
      "cosine similarity between orange and cantaloupe 0.589255650989\n",
      "cosine similarity between orange and peach 0.36514837167\n",
      "cosine similarity between orange and Phoenix 0.462910049886\n",
      "cosine similarity between cantaloupe and peach 0.645497224368\n",
      "cosine similarity between cantaloupe and Phoenix 0.436435780472\n",
      "cosine similarity between peach and Phoenix 0.507092552837\n"
     ]
    }
   ],
   "source": [
    "# reset the generator\n",
    "all_combinations = combinations(vocabulary_plus_oov, 2)\n",
    "# iterate through all words\n",
    "for (word1, word2) in all_combinations:\n",
    "    spelling_vector_1 = make_spelling_vector(word1)\n",
    "    spelling_vector_2 = make_spelling_vector(word2)\n",
    "    print(\"cosine similarity between {} and {}\".format(word1, word2), cos_sim(spelling_vector_1, spelling_vector_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "#### We've successfully generated similarity scores!  But...\n",
    "\n",
    "Do they really reflect anything semantic?  \n",
    "\n",
    "In other words, does it make sense that **\"peach\"** and **\"Phoenix\"** (`cosine similarity = 0.507`) <br>\n",
    "are **more** similar than **\"peach\"** and **\"orange\"** <br>\n",
    "(`cosine similarity = .365`)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Option 3: Word Embeddings\n",
    "Create a \"dense\" representation of each word where proximity in vector space represents \"similarity\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<img src=\"images/context.png\", width=1000,height=1000>\n",
    "https://blog.acolyer.org/2016/04/21/the-amazing-power-of-word-vectors/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<img src=\"images/architecture.png\", width=1000,height=1000>\n",
    "https://arxiv.org/pdf/1301.3781v3.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<img src=\"images/cbow.png\", width=600,height=600>\n",
    "https://blog.acolyer.org/2016/04/21/the-amazing-power-of-word-vectors/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "#### Using the `gensim` package in `python`\n",
    "https://radimrehurek.com/gensim/models/word2vec.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# load existing word2vec vectors into gensim\n",
    "\n",
    "# most frequent 125k words in Gigaword corpus\n",
    "w2v = models.Word2Vec.load_word2vec_format(fname=\"Gigaword_pruned_vectors.txt.gz\", binary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Pre-trained word embeddings can be loaded into `gensim` in `.txt` or `.txt.gz` format *as long as* the first line identifies (1) the number of words in file and (2) the dimensions of the vector\n",
    " \n",
    "```\n",
    "199999 200\n",
    "and -0.065843 -0.133472 0.020263 0.102796 0.003295 0.025878 -0.071714 0.054211 -0.026698 -0.036176 -0.024954 0.042049 -0.165819 -0.067038 0.117293 0.046338 0.012154 0.026929 -0.020248 0.120186 0.081922 0.062471 -0.063391 -0.048321 -0.108106 -0.067974 0.092109 -0.034439 -0.024319 0.008799 -0.099953\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.06338   , -0.146809  ,  0.110004  , -0.01205   , -0.045637  ,\n",
       "       -0.02224   , -0.045153  ,  0.079144  , -0.027216  , -0.027647  ,\n",
       "       -0.000434  ,  0.108648  , -0.060456  , -0.129502  ,  0.010897  ,\n",
       "        0.055499  ,  0.086099  ,  0.055282  ,  0.007365  ,  0.167188  ,\n",
       "        0.016705  ,  0.0744    , -0.07096   , -0.105974  , -0.095631  ,\n",
       "        0.006107  ,  0.12862299, -0.033055  , -0.020641  ,  0.024765  ,\n",
       "       -0.048181  , -0.090195  ,  0.007408  ,  0.073138  ,  0.031994  ,\n",
       "       -0.014252  ,  0.102764  , -0.081244  ,  0.10513   ,  0.039809  ,\n",
       "       -0.050727  ,  0.002429  , -0.01506   , -0.085081  , -0.02245   ,\n",
       "        0.102064  , -0.009099  , -0.092295  , -0.040276  ,  0.148752  ], dtype=float32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the first 50 dimensions of the vector for \"the\"\n",
    "w2v[\"the\"][0:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'abcdef'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-c371a7157d6c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mw2v\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"abcdef\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/mcapizzi/miniconda3/envs/word-embedding/lib/python3.5/site-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, words)\u001b[0m\n\u001b[1;32m   1502\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m             \u001b[0;31m# allow calls like trained_model['office'], as a shorthand for trained_model[['office']]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1504\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msyn0\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1505\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1506\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msyn0\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'abcdef'"
     ]
    }
   ],
   "source": [
    "w2v[\"abcdef\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def get_vector(word):\n",
    "    \"\"\"\n",
    "    Returns the word vector for that word or a vector of 0s for out-of-vocabulary\n",
    "    :param: word: word to lookup in vectors\n",
    "    :return: vector or vector of zeros\n",
    "    \"\"\"\n",
    "    # determine vector length\n",
    "    w2v_length = len(w2v[\"the\"])\n",
    "    # get vector\n",
    "    if word in w2v:\n",
    "            return w2v[word]\n",
    "    else:\n",
    "        return np.zeros((w2v_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_vector(\"abcdef\")[0:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "#### Evaluation of word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<img src=\"images/king_queen.png\", width=1000,height=1000>\n",
    "https://arxiv.org/pdf/1301.3781v3.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<img src=\"images/king_queen_vis.png\", width=1000,height=1000>\n",
    "https://www.aclweb.org/anthology/N/N13/N13-1090.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<img src=\"images/country_capital.png\", width=700,height=700>\n",
    "https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<img src=\"images/eval_1.png\", width=1000,height=1000>\n",
    "https://arxiv.org/pdf/1301.3781v3.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('queen', 0.6834795475006104), ('monarch', 0.6421915292739868), ('princess', 0.5896612405776978), ('beatrix', 0.5811704993247986), ('prince', 0.5663138031959534)]\n",
      "\n",
      "[('monkey', 0.4996635317802429), ('hamster', 0.48757076263427734), ('zombie', 0.4652235805988312), ('scoobydoo', 0.44669386744499207), ('piglet', 0.4450364112854004)]\n",
      "\n",
      "[('sister', 0.8335152268409729), ('daughter', 0.8259485960006714), ('mother', 0.7856060266494751), ('grandmother', 0.7708373069763184), ('sisterinlaw', 0.7601062655448914)]\n"
     ]
    }
   ],
   "source": [
    "# king - man + woman = queen\n",
    "sol = w2v.most_similar(\n",
    "    positive=['king', 'woman'], \n",
    "    negative=['man'], \n",
    "    topn=5)\n",
    "print(sol)\n",
    "print()\n",
    "\n",
    "# mouse - dollar + dollars = mice\n",
    "sol_2 = w2v.most_similar(\n",
    "    positive=['mouse', 'dollars'], \n",
    "    negative=['dollar'], \n",
    "    topn=5)\n",
    "print(sol_2)\n",
    "print()\n",
    "\n",
    "# brother - uncle + aunt = sister\n",
    "sol_3 = w2v.most_similar(\n",
    "    positive=['brother','aunt'],\n",
    "    negative=['uncle'],\n",
    "    topn=5)\n",
    "print(sol_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('monarch', 0.7166919708251953),\n",
       " ('princess', 0.7164901494979858),\n",
       " ('margrethe', 0.6889792680740356),\n",
       " ('beatrix', 0.6878944039344788),\n",
       " ('coronation', 0.6789792776107788),\n",
       " ('prince', 0.6730599403381348),\n",
       " ('wilhelmina', 0.6619384288787842),\n",
       " ('mettemarit', 0.6575925946235657),\n",
       " ('consort', 0.6492267847061157),\n",
       " ('duchess', 0.6444146633148193)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find most similar n words to a given word\n",
    "similar = w2v.similar_by_word(\"queen\", topn=10)\n",
    "similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('cat', 1.0),\n",
       " ('dog', 0.8524122834205627),\n",
       " ('puppy', 0.7896589040756226),\n",
       " ('pug', 0.783139169216156),\n",
       " ('critter', 0.7650502324104309),\n",
       " ('squirrel', 0.7516598701477051),\n",
       " ('feline', 0.7436362504959106),\n",
       " ('gerbil', 0.7435644865036011),\n",
       " ('monkey', 0.7434572577476501),\n",
       " ('hamster', 0.7323285341262817)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find  most similar n words to a given vector\n",
    "cat_vector = get_vector(\"cat\")\n",
    "cat_sim = w2v.similar_by_vector(cat_vector, topn=10)\n",
    "cat_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cereal'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find which word doesn't match\n",
    "list_of_words = \"breakfast cereal dinner lunch\"\n",
    "\n",
    "doesnt_match = w2v.doesnt_match(list_of_words.split())\n",
    "doesnt_match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('bad', 0.7170573472976685),\n",
       " ('terrific', 0.7161434888839722),\n",
       " ('decent', 0.7018914222717285),\n",
       " ('lousy', 0.6984266042709351),\n",
       " ('wonderful', 0.6819486618041992),\n",
       " ('perfect', 0.6481753587722778),\n",
       " ('great', 0.6480209827423096),\n",
       " ('nice', 0.6281204223632812),\n",
       " ('darn', 0.623289942741394),\n",
       " ('fun', 0.6176395416259766)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this approach doesn't handle antonyms well\n",
    "# \"That movie was _______.\"\n",
    "\n",
    "w2v.similar_by_word(\"good\", topn=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "#### Bias in Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "![king_queen](images/king_queen_2.png)\n",
    "![programmer_homemaker](images/programmer_homemaker.png)\n",
    "https://arxiv.org/pdf/1607.06520v1.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"images/gender_bias.png\", width=1000,height=1000>\n",
    "https://arxiv.org/pdf/1607.06520v1.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "#### Links to available word embeddings\n",
    "\n",
    "[The \"original\" code for `word2vec`, and pre-trained vectors](https://code.google.com/archive/p/word2vec/)\n",
    "\n",
    "[Stanford's approach to word embeddings, and pre-trained vectors](http://nlp.stanford.edu/projects/glove/)\n",
    "\n",
    "[A modified approach to word embeddings (feeding dependency tuples to the neural network instead of words), and pre-trained vectors](https://levyomer.wordpress.com/2014/04/25/dependency-based-word-embeddings/)\n",
    "\n",
    "[Word embeddings from a particular historical period](http://nlp.stanford.edu/projects/histwords/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Links to papers\n",
    "\n",
    "The \"original\" three papers on `word2vec` by Mikolov:\n",
    "\n",
    " - [Efficient Estimation of Word Representations in Vector Space](http://arxiv.org/pdf/1301.3781v3.pdf)\n",
    "\n",
    " - [Distributed Representations of Words and Phrases and their Compositionality](https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf)\n",
    "\n",
    " - [Linguistic Regularities in Continuous Space Word Representations](https://www.aclweb.org/anthology/N/N13/N13-1090.pdf)\n",
    "\n",
    "\n",
    "[Further analysis of approaches to word embeddings and their hyperparameters](https://transacl.org/ojs/index.php/tacl/article/viewFile/570/124)\n",
    "\n",
    "[Detailed evaluation of word embeddings](https://arxiv.org/pdf/1608.04207v1.pdf)\n",
    "\n",
    "[Website for evaluating word embeddings](http://veceval.com/)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Links to blogs\n",
    "\n",
    "[A good overview of NLP](https://blog.monkeylearn.com/the-definitive-guide-to-natural-language-processing/)\n",
    "\n",
    "[Blog post summary of the three \"original\" papers by Mikolov](https://blog.acolyer.org/2016/04/21/the-amazing-power-of-word-vectors/)\n",
    "\n",
    "[Detailed blog post on the application of word embeddings to analogies](https://quomodocumque.wordpress.com/2016/01/15/messing-around-with-word2vec/)\n",
    "\n",
    "[Appyling word embeddings to computer logs](https://gab41.lab41.org/three-things-we-learned-about-applying-word-vectors-to-computer-logs-c199070f390b#.k2mirf2oa)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
